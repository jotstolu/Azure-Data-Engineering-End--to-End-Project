# Azure-Data-Engineering-Project
This End to End Project involves Data Ingestion, Data Storage, Data Processing / ETL Pipelines, Data Transformation & Cleaning, Data Orchestration & Automation and Data Delivery. This project makes use of the popular Netflix dataset, which contains the main source file (Netflix_titles) and four other look up tables - Netflix directors, Netflix category, Netflix cast, and Netflix countries. 
A Medallion architecture was used to structure and organize data effectively:
- Bronze Layer: Raw data ingested directly from the source and Github repository API, stored in Delta format.
- Silver Layer: Cleaned data, removing duplicates and handling missing values, ensuring itâ€™s ready for analytics.
- Gold Layer: Aggregated and enriched data tailored to specific business needs

## PROJECT ARCHICTECTURE
![project_architecture](https://private-user-images.githubusercontent.com/121890747/453922655-d477b71f-1c48-49cc-813c-f3f8d273f512.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk2NDczNjUsIm5iZiI6MTc0OTY0NzA2NSwicGF0aCI6Ii8xMjE4OTA3NDcvNDUzOTIyNjU1LWQ0NzdiNzFmLTFjNDgtNDljYy04MTNjLWYzZjhkMjczZjUxMi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxMVQxMzA0MjVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05ZjQ0MGQ4NzBlODg5OGNmM2Y3YzVjZjQ3NzE1MGRlYTdiM2QzNjRlNjliYmQ4NWJmZDZlMjAwYzdiZGI2NjkzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.o_AOy24VHabxYMCIH92hLKyjefmbsJxFLNmOOg0tbFg)



#### **1) PHASE 1**
This phase involved data ingestion and storage from multiple sources - github repository and a source folder into the Bronze container. The netflix titles, which was the main source was manually uploaded as a csv file into a source folder titled rawdata in ADLS gen 2 while other lookup tables - Netflix directors, neflix cast, netflix countries and netflix category were pulled using Azure data factory. A dynamic and parameterized data pipelines were orchestrated using activities such as for each activity, if condition, validation, copy data activity to pull these data, which were saved as delta format into the bronze container. The netflix title data was incrementally pulled using databrick Autoloader from the rawdata folder into the bronze container, this was also saved as delta format
   
![adf_data_pipeline](https://private-user-images.githubusercontent.com/121890747/453949832-92fd0b31-0ee6-4218-9580-efebdbfe61cd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk2NTA3MDYsIm5iZiI6MTc0OTY1MDQwNiwicGF0aCI6Ii8xMjE4OTA3NDcvNDUzOTQ5ODMyLTkyZmQwYjMxLTBlZTYtNDIxOC05NTgwLWVmZWJkYmZlNjFjZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxMVQxNDAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00YTgxYWUwYjk1YTc5NTgzZTYwZGQ0OWExMjhhZmNkMTY3NzczZWE5NzE5ZWVlMWM2ZTNmOWM3ZGE1OGU0ZDNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.guq5ZKv3XdIzAsVLl_E8E7tAtAJ83T8QMlpzkNzXZvY)

#### **2) PHASE 2**
This phase involved data transformation such as splitting of column, removing invalid values, filling of null values, casting of datatypes etc. pyspark was used as a transformation tools on microsoft Azure Databrick workspace. Databricks workflow was orchestrated to create dynamic and parameterized notebooks, which saved the transformed data into the silver layer 
![silver_workflow](https://private-user-images.githubusercontent.com/121890747/453961432-b53605ce-25fc-49b1-8275-8ca2972cd7c6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk2NTIwNDQsIm5iZiI6MTc0OTY1MTc0NCwicGF0aCI6Ii8xMjE4OTA3NDcvNDUzOTYxNDMyLWI1MzYwNWNlLTI1ZmMtNDliMS04Mjc1LThjYTI5NzJjZDdjNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxMVQxNDIyMjRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iOGY2MDA1ZDhiNGI4MzM1ZTYxMzA1OTBkZjhlYmE2NTgyZDUxOGIxNjhlMWY0ODg5YmI0N2FiNGUzMzkyZTIxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.NYu6v5r4DDLLVblCjWxVGhXvrImEe6qxXTDb02NFk4k)

#### **3) PHASE 3**
A Delta live table pipeline was created to validate the data quality of the data from the silver layer. Different expectations and actions were defined on the tables such as NOT NULL expectation, drop action etc. The pipeline was ran and data records which passed the expectations were processed into the gold container while the ones which didnt passed the expectations were dropped
![DLT](https://private-user-images.githubusercontent.com/121890747/453973262-1517a8c5-dce5-4000-ba5a-6ec1aa6f519b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk2NTM0OTYsIm5iZiI6MTc0OTY1MzE5NiwicGF0aCI6Ii8xMjE4OTA3NDcvNDUzOTczMjYyLTE1MTdhOGM1LWRjZTUtNDAwMC1iYTVhLTZlYzFhYTZmNTE5Yi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxMVQxNDQ2MzZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iNzc2M2UwZjc4NmMwZWFhYjM1OGZlOGY5YTNkM2U2NDVmZTQ4ZDgyMWU5YTM3YThlM2Q3OWFhYmE5Y2NjYWY0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.OLVjT-mD7Audhb2FiIW7cSIy_zpxie4GNZO0tEksZ8o)

